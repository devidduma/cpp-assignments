Reinforcement learning is a field of machine learning that is concerned about the following problem: how to make a learning agent, which might be a robot or a computer achieve or exceed human level proficiency in a given task. In a reinforcement learning problem there is always a learning agent and an environment with which the agent interacts. Beyond the agent and the environment, one can identify three main elements of a reinforcement learning system: a policy, a reward signal and a value function.

A policy defines the learning agent’s way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states. It corresponds to what in psychology would be called a set of stimulus–response rules or associations.

A reward signal defines the goal of a reinforcement learning problem. On each time step, the environment sends to the reinforcement learning agent a single number called the reward. The agent’s sole objective is to maximize the total reward it receives over the long run.

Whereas the reward signal indicates what is good in an immediate sense, a value function specifies what is good in the long run. Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state. There are two kinds of value functions, one called a V value that maps states to an expected reward and one called Q value that maps state-action pairs to expected rewards.

A clear limitation of reinforcement learning being able to solve very challenging problems is, or at least was, that it relies heavily on the concept of state, which informally put is a situation in which the agent can end up in a particular time. It can be represented by a set of features to that situation that are then used as input to the policy and value function. This is usually implemented in a tabular setting, the agent storing the possible states in a tabular representation internally in the form of arrays. Then the agent would compute the Q value function based on the current state and possible actions to predict the best possible action to take in that state.

This tabular representation of states requires a huge amount of memory in the agent to be used for the representation of states and limiting our ability to apply reinforcement learning in problems with huge number of states. It also does not allow us to get insight about which states are similar in nature or have some underlying similar pattern. The recognition of such patterns and compression of information was not possible in the computer science world until the use of neural networks exploded in 2015, demonstrating applications of such networks in problems in healthcare, speech and text processing, face or fingerprint recognition.

What we are going to discuss in this case study is the usage of such neural networks in the algorithms of reinforcement learning. This is going to push the boundaries of the field, allowing reinforcement learning algorithms to be applied in more complex problems which traditionally were impossible to solve.

Deep neural networks are conceptually quite simple. The building blocks of these networks are the neurons and the weights. The neurons and weights contain a value, which might be a floating point number. The neurons are organized in layers, and neurons in a layer are connected with weights. The weights are then trained on a dataset, their values iteratively improved so that the network can learn the hidden function of the dataset: given an input vector x what should be the output vector y?